{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project One "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded for VTI. Retrying in 310 seconds...\n",
      "Data for AAPL saved to Data/AAPL.csv\n",
      "Data for JNJ saved to Data/JNJ.csv\n",
      "Data for JPM saved to Data/JPM.csv\n",
      "Data for MSFT saved to Data/MSFT.csv\n",
      "Data for SPY saved to Data/SPY.csv\n",
      "Data for VTI saved to Data/VTI.csv\n",
      "Data for C:EURUSD saved to Data/EURUSD.csv\n",
      "Data for C:USDJPY saved to Data/USDJPY.csv\n",
      "Data for I:NDX saved to Data/NDX.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "BASE_URL = 'https://api.polygon.io'\n",
    "\n",
    "def fetch_stock_data(ticker, start_date, end_date, retries=5, delay=310):\n",
    "    \"\"\"\n",
    "    Fetch historical stock data from Polygon.io API with retries and delay.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?apiKey={API_KEY}\"\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json().get('results', [])\n",
    "            return pd.DataFrame(data)\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded for {ticker}. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "        else:\n",
    "            print(f\"Error fetching data for {ticker}: {response.status_code}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def fetch_data_for_portfolio(assets, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch data for all assets in the portfolio.\n",
    "    \"\"\"\n",
    "    portfolio_data = {}\n",
    "    for asset_type, ticker in assets.items():\n",
    "        data = fetch_stock_data(ticker, start_date, end_date)\n",
    "        if data is not None:\n",
    "            portfolio_data[ticker] = data\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {ticker}\")\n",
    "    return portfolio_data\n",
    "\n",
    "def save_data_to_csv(portfolio_data):\n",
    "    \"\"\"\n",
    "    Save the fetched data to CSV files in the 'Data' folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('Data'):\n",
    "        os.makedirs('Data')\n",
    "    \n",
    "    for ticker, data in portfolio_data.items():\n",
    "        # Remove 'C:' or 'I:' prefix for file naming\n",
    "        filename = f\"Data/{ticker.replace('C:', '').replace('I:', '')}.csv\"\n",
    "        data.to_csv(filename, index=False)\n",
    "        print(f\"Data for {ticker} saved to {filename}\")\n",
    "\n",
    "# Define the assets in the portfolio\n",
    "assets = {\n",
    "    \"Stock1\": \"AAPL\",\n",
    "    \"Stock2\": \"JNJ\",\n",
    "    \"Stock3\": \"JPM\",\n",
    "    \"Stock4\": \"MSFT\",\n",
    "    \"ETF1\": \"SPY\",\n",
    "    \"ETF2\": \"VTI\",\n",
    "    \"Forex1\": \"C:EURUSD\",\n",
    "    \"Forex2\": \"C:USDJPY\",\n",
    "    \"MarketIndex\": \"I:NDX\"\n",
    "}\n",
    "\n",
    "# Define the date range for fetching data\n",
    "start_date = '2023-01-01'\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')  # Using today's date\n",
    "\n",
    "# Fetch data for all portfolio components\n",
    "portfolio_data = fetch_data_for_portfolio(assets, start_date, end_date)\n",
    "\n",
    "# Save the fetched data to CSV files\n",
    "save_data_to_csv(portfolio_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing cleaned data in Cleaned_Data\n",
      "Cleaned data for AAPL saved to Cleaned_Data/AAPL.csv\n",
      "Cleaned and processed data for AAPL:\n",
      "          Date       Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-03  112117471.0                125.7250    130.280      125.07   \n",
      "1  2023-01-04   89100633.0                126.6464    126.890      126.36   \n",
      "2  2023-01-05   80716808.0                126.0883    127.130      125.02   \n",
      "3  2023-01-06   87754715.0                128.1982    126.010      129.62   \n",
      "4  2023-01-09   70790813.0                131.6292    130.465      130.15   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0      130.9000       124.17  1672722000000               1021065  \n",
      "1      128.6557       125.08  1672808400000                770042  \n",
      "2      127.7700       124.76  1672894800000                665458  \n",
      "3      130.2900       124.89  1672981200000                711520  \n",
      "4      133.4100       129.89  1673240400000                645365  \n",
      "Cleaned data for JNJ saved to Cleaned_Data/JNJ.csv\n",
      "Cleaned and processed data for JNJ:\n",
      "          Date     Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-03  6344864.0                177.6642     176.16      178.19   \n",
      "1  2023-01-04  9788784.0                179.5564     178.89      180.13   \n",
      "2  2023-01-05  6255308.0                179.0114     179.02      178.80   \n",
      "3  2023-01-06  5705960.0                180.0545     180.13      180.25   \n",
      "4  2023-01-09  7925277.0                176.7125     179.30      175.58   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0        178.38      176.010  1672722000000                 69196  \n",
      "1        180.19      178.480  1672808400000                103756  \n",
      "2        179.99      178.410  1672894800000                 82270  \n",
      "3        180.93      179.390  1672981200000                 74837  \n",
      "4        179.76      175.175  1673240400000                 98621  \n",
      "Cleaned data for JPM saved to Cleaned_Data/JPM.csv\n",
      "Cleaned and processed data for JPM:\n",
      "          Date      Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-03  11054778.0                135.1124    135.240      135.12   \n",
      "1  2023-01-04  11687643.0                136.4908    135.990      136.38   \n",
      "2  2023-01-05   8381265.0                134.9880    135.660      135.35   \n",
      "3  2023-01-06  10029076.0                137.3405    136.125      137.94   \n",
      "4  2023-01-09   8482297.0                137.7288    138.600      137.37   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0        136.74     133.8900  1672722000000                115529  \n",
      "1        137.68     135.5700  1672808400000                130573  \n",
      "2        135.71     133.7004  1672894800000                 92590  \n",
      "3        138.38     134.4900  1672981200000                115516  \n",
      "4        138.88     136.8800  1673240400000                 96203  \n",
      "Cleaned data for MSFT saved to Cleaned_Data/MSFT.csv\n",
      "Cleaned and processed data for MSFT:\n",
      "          Date      Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-03  25740036.0                239.8394    243.080      239.58   \n",
      "1  2023-01-04  50623394.0                228.5518    232.275      229.10   \n",
      "2  2023-01-05  39577823.0                223.6040    227.200      222.31   \n",
      "3  2023-01-06  43613574.0                223.3788    223.000      224.93   \n",
      "4  2023-01-09  27369784.0                228.5989    226.450      227.12   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0      245.7500     237.4000  1672722000000                314904  \n",
      "1      232.8700     225.9600  1672808400000                679231  \n",
      "2      227.5500     221.7602  1672894800000                471302  \n",
      "3      225.7601     219.3500  1672981200000                495762  \n",
      "4      231.2366     226.4100  1673240400000                363680  \n",
      "Cleaned data for SPY saved to Cleaned_Data/SPY.csv\n",
      "Cleaned and processed data for SPY:\n",
      "          Date       Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-03   74850731.0                380.9576     384.37      380.82   \n",
      "1  2023-01-04   85934098.0                383.1494     383.18      383.76   \n",
      "2  2023-01-05   76275354.0                380.2625     381.72      379.38   \n",
      "3  2023-01-06  104052662.0                385.2463     382.61      388.08   \n",
      "4  2023-01-09   73978071.0                390.3628     390.37      387.86   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0        386.43     377.8310  1672722000000                590240  \n",
      "1        385.88     380.0000  1672808400000                632808  \n",
      "2        381.84     378.7600  1672894800000                530896  \n",
      "3        389.25     379.4127  1672981200000                687390  \n",
      "4        393.70     387.6700  1673240400000                549428  \n",
      "Cleaned data for VTI saved to Cleaned_Data/VTI.csv\n",
      "Cleaned and processed data for VTI:\n",
      "          Date     Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-03  4092466.0                190.3996    192.380      190.41   \n",
      "1  2023-01-04  3852482.0                191.7975    191.510      192.10   \n",
      "2  2023-01-05  4149991.0                190.2798    191.220      189.85   \n",
      "3  2023-01-06  3184261.0                192.8493    191.120      194.04   \n",
      "4  2023-01-09  3888341.0                195.6597    195.457      194.12   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0      193.3600      188.930  1672722000000                 96002  \n",
      "1      193.0600      190.140  1672808400000                 75365  \n",
      "2      191.2700      189.460  1672894800000                 66699  \n",
      "3      194.6400      189.850  1672981200000                 60241  \n",
      "4      196.8882      194.008  1673240400000                 60835  \n",
      "Cleaned data for C:EURUSD saved to Cleaned_Data/EURUSD.csv\n",
      "Cleaned and processed data for C:EURUSD:\n",
      "          Date  Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-01      12                  1.0691    1.06849     1.07930   \n",
      "1  2023-01-02   74147                  1.0676    1.07027     1.06764   \n",
      "2  2023-01-03  236637                  1.0585    1.06765     1.05450   \n",
      "3  2023-01-04  229494                  1.0598    1.05453     1.06050   \n",
      "4  2023-01-05  210543                  1.0574    1.06056     1.05211   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0       1.07930      1.06788  1672531200000                    12  \n",
      "1       1.07100      1.06450  1672617600000                 74147  \n",
      "2       1.06810      1.05180  1672704000000                236637  \n",
      "3       1.06353      1.05440  1672790400000                229494  \n",
      "4       1.06320      1.05138  1672876800000                210543  \n",
      "Cleaned data for C:USDJPY saved to Cleaned_Data/USDJPY.csv\n",
      "Cleaned and processed data for C:USDJPY:\n",
      "          Date  Volume  VolumeWeightedAvgPrice  OpenPrice  ClosePrice  \\\n",
      "0  2023-01-01      12                130.8361    130.952     130.845   \n",
      "1  2023-01-02   81379                130.8025    130.860     130.795   \n",
      "2  2023-01-03  276808                130.4668    130.787     131.293   \n",
      "3  2023-01-04  272444                131.1816    131.295     132.252   \n",
      "4  2023-01-05  272141                132.8809    132.244     133.317   \n",
      "\n",
      "   HighestPrice  LowestPrice  UnixTimestamp  NumberOfTransactions  \n",
      "0       130.952      130.732  1672531200000                    12  \n",
      "1       131.402      130.529  1672617600000                 81379  \n",
      "2       131.445      129.500  1672704000000                276808  \n",
      "3       132.720      129.923  1672790400000                272444  \n",
      "4       134.051      131.681  1672876800000                272141  \n",
      "Cleaned data for I:NDX saved to Cleaned_Data/NDX.csv\n",
      "Cleaned and processed data for I:NDX:\n",
      "          Date     OpenPrice    ClosePrice  HighestPrice   LowestPrice  \\\n",
      "0  2023-02-22  12085.674327  12066.271823  12156.213862  12006.037547   \n",
      "1  2023-02-24  11979.799425  11969.651998  12018.320636  11900.838503   \n",
      "2  2023-02-27  12106.791649  12057.788354  12159.638149  12034.612553   \n",
      "3  2023-02-28  12041.746389  12042.116479  12146.523163  12021.320963   \n",
      "4  2023-03-01  12026.719702  11937.475278  12054.476681  11906.578816   \n",
      "\n",
      "   UnixTimestamp  \n",
      "0  1677045600000  \n",
      "1  1677218400000  \n",
      "2  1677477600000  \n",
      "3  1677564000000  \n",
      "4  1677650400000  \n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Step 3: Clean and Transform Data\n",
    "# -------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def load_data(ticker):\n",
    "    \"\"\"\n",
    "    Load data from CSV file in the 'Data' folder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filename = ticker.replace(\"C:\", \"\").replace(\"I:\", \"\")\n",
    "        df = pd.read_csv(f\"Data/{filename}.csv\")\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No data found for {ticker}.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"No data in CSV file for {ticker}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_data(df, ticker):\n",
    "    \"\"\"\n",
    "    Clean the data: handle missing values, normalize formats, rename columns.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Convert the Unix timestamp to a datetime object and remove time component\n",
    "    if 't' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['t'], unit='ms').dt.date\n",
    "    \n",
    "    # Rename columns for better readability\n",
    "    if ticker == 'I:NDX':\n",
    "        df.rename(columns={\n",
    "            'c': 'ClosePrice',\n",
    "            'h': 'HighestPrice',\n",
    "            'l': 'LowestPrice',\n",
    "            'o': 'OpenPrice',\n",
    "            't': 'UnixTimestamp'\n",
    "        }, inplace=True)\n",
    "        df = df[['Date', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp']]\n",
    "    else:\n",
    "        df.rename(columns={\n",
    "            'v': 'Volume',\n",
    "            'vw': 'VolumeWeightedAvgPrice',\n",
    "            'o': 'OpenPrice',\n",
    "            'c': 'ClosePrice',\n",
    "            'h': 'HighestPrice',\n",
    "            'l': 'LowestPrice',\n",
    "            'n': 'NumberOfTransactions',\n",
    "            't': 'UnixTimestamp'\n",
    "        }, inplace=True)\n",
    "        df = df[['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions']]\n",
    "    \n",
    "    # Drop rows with missing dates\n",
    "    df = df.dropna(subset=['Date'])\n",
    "    \n",
    "    # Drop unnecessary columns if they exist\n",
    "    df = df.drop(columns=['t'], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_cleaned_data(ticker, df):\n",
    "    \"\"\"\n",
    "    Save the cleaned data to CSV files in the 'Cleaned_Data' folder.\n",
    "    \"\"\"\n",
    "    cleaned_data_folder = \"Cleaned_Data\"\n",
    "    if not os.path.exists(cleaned_data_folder):\n",
    "        os.makedirs(cleaned_data_folder)\n",
    "    \n",
    "    filename = f\"{cleaned_data_folder}/{ticker.replace('C:', '').replace('I:', '')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Cleaned data for {ticker} saved to {filename}\")\n",
    "\n",
    "def clear_cleaned_data():\n",
    "    \"\"\"\n",
    "    Clear the 'Cleaned_Data' folder if it exists.\n",
    "    \"\"\"\n",
    "    cleaned_data_folder = \"Cleaned_Data\"\n",
    "    if os.path.exists(cleaned_data_folder):\n",
    "        shutil.rmtree(cleaned_data_folder)\n",
    "        print(f\"Cleared existing cleaned data in {cleaned_data_folder}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = [\"AAPL\", \"JNJ\", \"JPM\", \"MSFT\", \"SPY\", \"VTI\", \"C:EURUSD\", \"C:USDJPY\", \"I:NDX\"]\n",
    "    \n",
    "    # Clear previously cleaned data\n",
    "    clear_cleaned_data()\n",
    "    \n",
    "    cleaned_data = {}\n",
    "    for ticker in tickers:\n",
    "        df = load_data(ticker)\n",
    "        df = clean_data(df, ticker)\n",
    "        cleaned_data[ticker] = df\n",
    "        save_cleaned_data(ticker, df)\n",
    "        print(f\"Cleaned and processed data for {ticker}:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to SQL Server and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQL Server\n",
      "Inserted data into Cleaned_AAPL\n",
      "Inserted data into Cleaned_JNJ\n",
      "Inserted data into Cleaned_JPM\n",
      "Inserted data into Cleaned_MSFT\n",
      "Inserted data into Cleaned_SPY\n",
      "Inserted data into Cleaned_VTI\n",
      "Inserted data into Cleaned_EURUSD\n",
      "Inserted data into Cleaned_USDJPY\n",
      "Inserted data into Cleaned_NDX\n",
      "Data loading into SQL Server completed\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Load Data into SQL Server: Cleaned Data\n",
    "# -------------------\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Connection string\n",
    "conn_str = (\n",
    "    r'DRIVER={SQL Server};'\n",
    "    r'SERVER=IMPRINT;'\n",
    "    r'DATABASE=Project 1;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "# Connect to SQL Server\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Print a success message\n",
    "print(\"Connected to SQL Server\")\n",
    "\n",
    "# Function to load CSV data into SQL Server table\n",
    "def load_csv_to_sql(file_path, table_name, columns):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure DataFrame columns match SQL table columns\n",
    "    df.columns = columns\n",
    "    placeholders = ', '.join(['?' for _ in range(len(df.columns))])\n",
    "    columns_str = ', '.join(columns)\n",
    "\n",
    "    sql = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})\"\n",
    "    \n",
    "    for row in df.itertuples(index=False, name=None):\n",
    "        cursor.execute(sql, row)\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Inserted data into {table_name}\")\n",
    "\n",
    "# Define the cleaned data files and their corresponding SQL table names\n",
    "cleaned_data_files = {\n",
    "    'Cleaned_AAPL': 'Cleaned_Data/AAPL.csv',\n",
    "    'Cleaned_JNJ': 'Cleaned_Data/JNJ.csv',\n",
    "    'Cleaned_JPM': 'Cleaned_Data/JPM.csv',\n",
    "    'Cleaned_MSFT': 'Cleaned_Data/MSFT.csv',\n",
    "    'Cleaned_SPY': 'Cleaned_Data/SPY.csv',\n",
    "    'Cleaned_VTI': 'Cleaned_Data/VTI.csv',\n",
    "    'Cleaned_EURUSD': 'Cleaned_Data/EURUSD.csv',\n",
    "    'Cleaned_USDJPY': 'Cleaned_Data/USDJPY.csv',\n",
    "    'Cleaned_NDX': 'Cleaned_Data/NDX.csv'\n",
    "}\n",
    "\n",
    "# Define the columns for each table\n",
    "table_columns = {\n",
    "    'Cleaned_AAPL': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_JNJ': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_JPM': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_MSFT': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_SPY': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_VTI': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_EURUSD': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_USDJPY': ['Date', 'Volume', 'VolumeWeightedAvgPrice', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp', 'NumberOfTransactions'],\n",
    "    'Cleaned_NDX': ['Date', 'OpenPrice', 'ClosePrice', 'HighestPrice', 'LowestPrice', 'UnixTimestamp']\n",
    "}\n",
    "\n",
    "# Load CSV data into SQL Server cleaned tables\n",
    "for table_name, file_path in cleaned_data_files.items():\n",
    "    load_csv_to_sql(file_path, table_name, table_columns[table_name])\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Print a success message\n",
    "print(\"Data loading into SQL Server completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
